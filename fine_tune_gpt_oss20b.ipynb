{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO95EtEVI9/RhPbTHC8LHaK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smebellis/cis540_final_project/blob/main/fine_tune_gpt_oss20b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "SP21B0h-ppyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch --index-url https://download.pytorch.org/whl/cu128\n",
        "%pip install \"trl>=0.20.0\" \"peft>=0.17.0\" \"transformers>=4.55.0\"\n",
        "%pip install wandb\n",
        "%pip install bitsandbytes"
      ],
      "metadata": {
        "id": "hoAjd6LQprT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from datasets import load_dataset\n",
        "import wandb\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTConfig, SFTTrainer"
      ],
      "metadata": {
        "id": "eFy_UrbHp0OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Login to Hugging Face"
      ],
      "metadata": {
        "id": "lOprj0egFBPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "bDxh9bfNGM-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Weights and Biases"
      ],
      "metadata": {
        "id": "BRhjUgqZvbWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "2PeDyyUKGSNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the dataset"
      ],
      "metadata": {
        "id": "YDBKxPB5qtfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "lNZLn0vLp_rT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_PROJECT\"]=\"cis540_final_project\""
      ],
      "metadata": {
        "id": "ZjJt12D6wW3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PF6CgxskwScA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Tokenizer"
      ],
      "metadata": {
        "id": "a8PSKk9irJBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")"
      ],
      "metadata": {
        "id": "iGsQSSjvq2Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"You are a cyber threat intelligence model trained to identify and extract Indicators of\n",
        "        Compromise (IOCs) from unstructured text, logs, and threat reports. Your primary purpose is to detect patterns\n",
        "        related to advanced persistent threats (APTs) and output all identified indicators in a structured, normalized\n",
        "        format suitable for ingestion by security automation tools.\n",
        "\n",
        "Your output must always follow this exact structure:\n",
        "\n",
        "SHA256sum\n",
        "• <list of SHA-256 hashes>\n",
        "\n",
        "SHA1sum\n",
        "• <list of SHA-1 hashes>\n",
        "\n",
        "MD5sum\n",
        "• <list of MD5 hashes>\n",
        "\n",
        "IP Addresses\n",
        "• <list of IPv4 or IPv6 addresses>\n",
        "\n",
        "Each list item should:\n",
        "- Contain one indicator per line prefixed with a bullet (•).\n",
        "- Exclude duplicates and irrelevant tokens.\n",
        "- Maintain grouping order: SHA256 → SHA1 → MD5 → IP Addresses.\n",
        "\n",
        "Detection rules:\n",
        "- Identify cryptographic hashes: SHA-256 (64 hex chars), SHA-1 (40 hex chars), MD5 (32 hex chars)\n",
        "- Identify network indicators: IPv4/IPv6 addresses\n",
        "- Ignore false positives (short hex strings, malformed IPs)\n",
        "- Output nothing if no valid indicators are detected\n",
        "- Do NOT generate explanations, summaries, or context — only the IOC block.\n",
        "\n",
        "Example Input:\n",
        "\"The malware sample dropped a payload with SHA256 35a485972282b7e0e8e3a7a9cfb6ad9385637f8d96ce8e23 and communicated with 209.51.54.243.\"\n",
        "\n",
        "Example Output:\n",
        "What a Custom LLM Should Generate\n",
        "\n",
        "SHA256sum\n",
        "• 35a485972282b7e0e8e3a7a9cfb6ad9385637f8d96ce8e23\n",
        "\n",
        "IP Addresses\n",
        "• 209.51.54.243\"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "conversation = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "print(conversation)\n"
      ],
      "metadata": {
        "id": "J7Dlj5NHpF7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# messages = dataset[0][\"messages\"]\n",
        "# conversation = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "# print(conversation)"
      ],
      "metadata": {
        "id": "OXVSltrGrV7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the Model"
      ],
      "metadata": {
        "id": "ibcm6uvcvvG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    use_cache=False,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"openai/gpt-oss-20b\", **model_kwargs)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\", **model_kwargs)"
      ],
      "metadata": {
        "id": "6fWNkX7bvxZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"¿Cuál es el capital de Australia?\"},\n",
        "]\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "output_ids = model.generate(input_ids, max_new_tokens=512)\n",
        "response = tokenizer.batch_decode(output_ids)[0]\n",
        "print(response)"
      ],
      "metadata": {
        "id": "1SqSJ_9QrVvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=\"all-linear\",\n",
        "    target_parameters=[\n",
        "        \"7.mlp.experts.gate_up_proj\",\n",
        "        \"7.mlp.experts.down_proj\",\n",
        "        \"15.mlp.experts.gate_up_proj\",\n",
        "        \"15.mlp.experts.down_proj\",\n",
        "        \"23.mlp.experts.gate_up_proj\",\n",
        "        \"23.mlp.experts.down_proj\",\n",
        "    ],\n",
        ")\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "JhMw8a8rrVhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning"
      ],
      "metadata": {
        "id": "9TVjUY1gxRrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = SFTConfig(\n",
        "    learning_rate=2e-4,\n",
        "    gradient_checkpointing=True,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=1,\n",
        "    per_device_train_batch_size=1, # Reduced batch size\n",
        "    gradient_accumulation_steps=16, # Increased accumulation steps to compensate for smaller batch size\n",
        "    max_length=2048,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
        "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
        "    output_dir=\"gpt-oss-20b-multilingual-reasoner\",\n",
        "    report_to=\"wandb\",\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "qMp--MMdxTeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "1nNZYyQGxPwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push Model to HuggingFace Hub"
      ],
      "metadata": {
        "id": "4VmM7l_Dp-fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(training_args.output_dir)\n",
        "trainer.push_to_hub(dataset_name=\"HuggingFaceH4/Multilingual-Thinking\")"
      ],
      "metadata": {
        "id": "ARsnwXUzxPm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i6J4nJR0qB3Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}